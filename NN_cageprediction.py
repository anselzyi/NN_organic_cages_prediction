# -*- coding: utf-8 -*-
"""kj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qj_JDDsgSL8qpYPcaaYeaKi9AdY-lIwZ
"""

# import necessary packages
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.utils import shuffle 
from sklearn.metrics import classification_report
import re

np.random.seed(1)

#pd.set_option('display.max_rows',None)
#pd.set_option('display.max_columns',None)

# import dataset
dataset = pd.read_csv('cages.csv')
dataset.head()

# take out the reaction type 'amine2aldehyde3' and topology type 'FourPlusSix'
subdataset = dataset[dataset['reaction'] == 'amine2aldehyde3']
subdataset1 = subdataset[subdataset['topology'] == 'FourPlusSix']

# eliminate the nan data
subdataset2 = subdataset1[subdataset1['collapsed'].notnull()]
subdataset3 = subdataset2['fingerprint']


# build the overall training set
fingerprints =[]
single_fingerprint = []

# split the string by commas
# transfer string to float32
for i in range(0,len(subdataset3)):
    single_fingerprint = subdataset3.iloc[i,].split(',')
    for i in range(0, len(single_fingerprint)):
        single_fingerprint[i] = re.findall("\d+", single_fingerprint[i])
        single_fingerprint[i] = [float(x) for x in single_fingerprint[i]]
    fingerprints.append(single_fingerprint)

fingerprints = np.squeeze(np.array(fingerprints)) # to array and reduce the dimension
#print(fingerprints)

# build the labels
labels_all = subdataset2['collapsed'].values
#print(labels)

num_dataset = len(fingerprints)
#print(num_dataset)
dataset_index = np.arange(num_dataset) 

# divide the dataset by labels
labels_zero_index = list(np.where(labels_all == 0))
labels_one_index = list(np.where(labels_all == 1))
#print(np.array(labels_zero_index).size)
#print(np.array(labels_one_index).size)

fingerprints_zero = fingerprints[labels_zero_index]
fingerprints_one = fingerprints[labels_one_index]
labels_zero = labels_all[labels_zero_index]
labels_one = labels_all[labels_one_index]

# shuffle the datasets
num_dataset_zero = len(fingerprints_zero)
num_dataset_one = len(fingerprints_one)
fingerprints_zero_index = np.arange(num_dataset_zero)
fingerprints_one_index = np.arange(num_dataset_one)

shuffle_fingerprints_zero_index = shuffle(fingerprints_zero_index)
shuffle_fingerprints_one_index = shuffle(fingerprints_one_index)

proportion_test = 0.2 # 80% train_dev set, 20% test set
num_fingerprints_zero_test = int(proportion_test * num_dataset_zero)
num_fingerprints_one_test = int(proportion_test * num_dataset_one)

# build the test set and test label
index_fingerprints_zero_test = shuffle_fingerprints_zero_index[:num_fingerprints_zero_test]
index_fingerprints_one_test = shuffle_fingerprints_one_index[: num_fingerprints_one_test]

fingerprints_zero_test = fingerprints_zero[index_fingerprints_zero_test]
fingerprints_one_test = fingerprints_one[index_fingerprints_one_test]
label_zero_test = labels_zero[index_fingerprints_zero_test]
label_one_test = labels_one[index_fingerprints_one_test]

test_set = np.concatenate((fingerprints_zero_test, fingerprints_one_test), axis = 0)
test_label = np.concatenate((label_zero_test, label_one_test), axis = 0)
#print(test_set.shape)
#print(test_label.shape)

# build the train_dev set and train_dev label
index_train_dev_zero = shuffle_fingerprints_zero_index[num_fingerprints_zero_test:]
index_train_dev_one = shuffle_fingerprints_one_index[num_fingerprints_one_test:]
fingerprints_zero_train_dev = fingerprints_zero[index_train_dev_zero]
fingerprints_one_train_dev = fingerprints_one[index_train_dev_one]
label_zero_train_dev = labels_zero[index_train_dev_zero]
label_one_train_dev = labels_one[index_train_dev_one]

train_dev_set = np.concatenate((fingerprints_zero_train_dev, fingerprints_one_train_dev), axis = 0)
train_dev_label = np.concatenate((label_zero_train_dev, label_one_train_dev), axis = 0)
#print(train_dev_set.shape)
#print(train_dev_label.shape)

# evenly distribute data in test and train_dev set
num_test = len(test_set)
num_train_dev = len(train_dev_set)
test_index = np.arange(num_test)
train_dev_index = np.arange(num_train_dev)

shuffle_num_test = shuffle(test_index)
shuffle_num_train_dev = shuffle(train_dev_index)

# final version of test set and train_dev set
test_set = test_set[shuffle_num_test]
test_label = test_label[shuffle_num_test][:, np.newaxis]    # shuffle and add one dimension

train_dev_set = train_dev_set[shuffle_num_train_dev]
train_dev_label = train_dev_label[shuffle_num_train_dev][:, np.newaxis]   # shuffle and add one dimension

# transfer test set and train_dev set from array to tensor
test_set = torch.from_numpy(test_set).float()
test_label = torch.from_numpy(test_label).float()

train_dev_set = torch.from_numpy(train_dev_set).float() 
train_dev_label = torch.from_numpy(train_dev_label).float()

print(test_set.shape)
print(test_label.shape)
print(train_dev_set.shape)
print(train_dev_label.shape)

# construct the neural network
class collapse_model(nn.Module):
    def __init__(self, x_size, hidden1_size, hidden2_size, hidden3_size, y_size):
        super(collapse_model, self).__init__()
        self.hidden1 = nn.Linear(x_size, hidden1_size)
        #self.batch1 = nn.BatchNorm1d(hidden1_size)   # add batch norm 1
        self.hidden2 = nn.Linear(hidden1_size, hidden2_size)
        #self.batch2 = nn.BatchNorm1d(hidden2_size)  # add batch norm 2
        self.hidden3 = nn.Linear(hidden2_size, hidden3_size)
        self.predict = nn.Linear(hidden3_size, y_size)
    def forward(self, input):
        result = self.hidden1(input)
        #result = self.batch1(result)   # add batch norm 1
        result = F.leaky_relu(result)
        result = self.hidden2(result)
        #result = self.batch2(result)   # add batch norm 2
        result = F.leaky_relu(result)
        result = self.hidden3(result)
        result = torch.sigmoid(result)
        result = self.predict(result)
        
        return result

net = collapse_model(1024, 512, 256, 64, 1)
print(net)

# set optimizer and loss function
#optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) # 87.10%
#optimizer = optim.Adam(net.parameters(), lr=0.001, eps=1e-08, weight_decay=0) # 87.32
#optimizer = optim.Adam(net.parameters(), lr=0.002, eps=1e-08, weight_decay=0) # 87.21
optimizer = optim.Adam(net.parameters(), lr=0.00001, eps=1e-08, weight_decay=0)
loss_func = nn.BCEWithLogitsLoss()

# use GPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
train_dev_set = train_dev_set.to(device)
train_dev_label = train_dev_label.to(device)
test_set = test_set.to(device)
test_label = test_label.to(device)

net = net.to(device)
loss_func = loss_func.to(device)

# training and evaluation
def evaluate_accuracy(y, pred):
    num_set = len(y)

    predicted = pred.ge(0.5).view(-1)  
    performance = (y == predicted).sum().float() / num_set

    
    return performance

def train(net, train_x, train_y, test_x, test_y, loss, num_epochs, optimizer):

    for epoch in range(num_epochs):

        pred = net(train_x)
        pred = torch.squeeze(pred)
        l = loss_func(pred, train_y)
        #optimizer.zero_grad()
        #l.backward()
        #optimizer.step()
        train_loss = l.item()


        if epoch == 0 or (epoch + 1) % 100 == 0:
            train_acc = evaluate_accuracy(train_y, pred)

            test_pred = net(test_x)
            test_pred = torch.squeeze(test_pred)
            test_loss = loss_func(test_pred, test_y)
            test_acc = evaluate_accuracy(test_y, test_pred)
            test_loss += test_loss.item()

            print('epoch %d ,train loss %.4f'%(epoch + 1,train_loss)+', train accuracy {:.2f}%'.format(train_acc*100))
            print('test loss %.4f'% (test_loss)+' test accuracy {:.2f}%'.format(test_acc*100))
        
        optimizer.zero_grad()
        l.backward()
        optimizer.step()

num_epochs = 3000
train(net, train_dev_set, torch.squeeze(train_dev_label), test_set, torch.squeeze(test_label), loss_func, num_epochs, optimizer)

#evaluation
classes = ['collapsed: 0','collapsed: 1']
y_pred = net(test_set)
y_pred = y_pred.ge(.5).view(-1).cpu()
y_test = torch.squeeze(test_label).cpu()
print(classification_report(y_test, y_pred, target_names=classes))

#from google.colab import drive
#drive.mount('/drive')

#%cd ..
#%cd drive/MyDrive/Colab_Notebooks/kimjelfs



